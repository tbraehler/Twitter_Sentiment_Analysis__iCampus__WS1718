{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Twitter Sentiment Analysis  +  iCampus-Seminar  +  WS1718\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following program catches Tweets from Twitter and assigns an emotion based on the text using a Neural Network. <br>\n",
    "For that, the program covers catching the Tweets, preprocessing the Tweets, training a Neural Network, scoring new Tweets based on the trained model and plotting the result.\n",
    "\n",
    "This notebook is structured as following:\n",
    "1. [Imports](#imports) and [Global Variables](#globalvar)\n",
    "2. Main functions to control the execution flow:\n",
    "\t2. [Function](#maincatch) to catch training tweets based on a given query.\n",
    "\t2. [Function](#maintrain) to train the Neural Network model.\n",
    "\t2. [Function](#mainscore) to score emotions of tweets and plot spider graphs.\n",
    "3. Classes  which contains the logic:\n",
    "\t3. [Class](#database) to create the Database-API and manage the Database.\n",
    "\t3. [Class](#twitter) to create the Twitter-API and to load Tweets based on a given search query.\n",
    "\t3. [Class](#preprocessing) to preprocess a given Tweet. \n",
    "\t3. [Class](#train) to train the Neural Network.\n",
    "\t3. [Class](#score) to score a given text based on the trained Neural Network.\n",
    "\t3. [Class](#plot) to plot the emotions as a spider graph.\n",
    "4. Helper classes\n",
    "\t4. Helper [Class](#helptweets) to get informations about Tweets in database.\n",
    "\t4. Helper [Class](#helpdelete) to delete a given Database.\n",
    "\n",
    "<br>\n",
    "¬© Tobias Br√§hler | tobias.braehler@mni.thm.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"imports\"></a>\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYS\n",
    "from time import strftime\n",
    "from datetime import datetime\n",
    "from typing import Dict, Union, List, Tuple\n",
    "from sys import stdout\n",
    "from re import sub\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "# MATH\n",
    "from math import pi\n",
    "\n",
    "# MATPLOT\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PANDAS + NUMPY\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PYMONGO\n",
    "from pymongo import MongoClient, DESCENDING\n",
    "from pymongo.errors import ConnectionFailure, BulkWriteError\n",
    "from pymongo.cursor import Cursor\n",
    "\n",
    "# TWEEPY\n",
    "from tweepy import AppAuthHandler, API, Cursor as Tweepy_Cursor, TweepError\n",
    "\n",
    "# GENSIM + WORD2VEC\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.doc2vec import FAST_VERSION\n",
    "\n",
    "# NLT\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# SCIKIT-LEARN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# KERAS\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import metrics\n",
    "\n",
    "# TQDM\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"globalvar\"></a>\n",
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_db_collection_training = \"training_tweets\"\n",
    "_db_collection_production = \"production_tweets\"\n",
    "\n",
    "# search term; filter out retweets\n",
    "_twitter_search_query = \"#realDonaldTrump lang:en -filter:retweets\"\n",
    "\n",
    "_trained_model_savepath = 'C:\\\\Users\\\\admin\\\\Desktop\\\\twitter_sentiment_model_%s.h5' \\\n",
    "                          % strftime('%d%m%Y-%H%M')\n",
    "_classlabels_savepath = 'C:\\\\Users\\\\admin\\\\Desktop\\\\classlabels.json'\n",
    "\n",
    "_db_client_train = Database(db_collection=_db_collection_training)\n",
    "_db_client_production = Database(db_collection=_db_collection_production)\n",
    "_twitter_client = Twitter()\n",
    "_preprocessing = Preprocessing()\n",
    "_training = Training()\n",
    "_score = Score(model=_trained_model_savepath, classlabels=_classlabels_savepath)\n",
    "\n",
    "_word2vec = KeyedVectors.load_word2vec_format('C:/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"maincatch\"></a>\n",
    "### MAIN: Load Training-Tweets based on a given search query.\n",
    "- Catch Tweets from Twitter using the [Twitter-Class](#twitter)\n",
    "- Preprocess each Tweet using the [Preprocess-Class](#preprocessing)\n",
    "- Load Tweets into Database using the [Database-Class](#database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catch_training_tweets_from_twitter():\n",
    "        print('\\n', '[', strftime(\"%d.%m.%Y %H:%M:%S\"), ']', ' Start mining Tweets ...', sep='')\n",
    "        print('Exit with CTRL+C', '\\n')\n",
    "\n",
    "        # get since_id; 'None' if no Tweet is persisted yet; needed to stop catching Tweets if all \n",
    "        # Tweets are catched back to the newest Tweet persisted in database (Tweepy catches Tweets \n",
    "        # from most recent to oldest)\n",
    "        __since_id = _db_client_train.get_newest_tweet_id(tweet_id_column='id')\n",
    "        __max_id = None\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                # get tweets\n",
    "                tweets = _twitter_client.load_tweets(search_query=_twitter_search_query,\n",
    "                                                     since_id=__since_id,\n",
    "                                                     max_id=__max_id)\n",
    "                if not tweets:\n",
    "                    break\n",
    "\n",
    "                # set max_id, i.e. lowest tweet id completely processed so far in this run.\n",
    "                # note: tweepy is going from recent to oldest, therefore, the last tweet processed \n",
    "                # is the oldest so far.\n",
    "                __max_id = tweets[-1]['id'] - 1\n",
    "\n",
    "                # preprocess tweets for training; add to list if its not 'None'\n",
    "                processed_tweets = []\n",
    "                for tweet in tweets:\n",
    "                    tweet = _preprocessing.preprocess_tweet_for_training(tweet)\n",
    "                    if tweet:\n",
    "                        processed_tweets.append(tweet)\n",
    "\n",
    "                # persist tweets in database\n",
    "                # Note: If the items tweet[\"user\"][\"id\"] or tweet[\"created_at_week_year\"] change \n",
    "                # (e.g. information are stored elsewhere, the function in class \"Database\" has to \n",
    "                # be updated.\n",
    "                _db_client_train.persist_tweets(tweets=processed_tweets)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "catch_training_tweets_from_twitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"maintrain\"></a>\n",
    "### MAIN: Train the Neural Network Model.\n",
    "- Get training Tweets using the [Database-Class](#database)\n",
    "- Train the model using the [Train-Class](#train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(limit: int = 100000):\n",
    "        # get training data\n",
    "        __data = _db_client_train.get_trainingdata(limit=limit)\n",
    "        # split training data into training and test\n",
    "        __text_train, __text_test, __label_train, __label_test = \\\n",
    "            _training._split_into_test_and_training(__data)\n",
    "        # convert data to embedded vectors resp. binary labels\n",
    "        __text_train_embedvec = _training._texts_to_embedded_vec_matrices(__text_train)\n",
    "        __text_test_embedvec = _training._texts_to_embedded_vec_matrices(__text_test)\n",
    "        __label_train_binary = _training._labels_to_binary(__label_train)\n",
    "        __label_test_binary = _training._labels_to_binary(__label_test)\n",
    "        # train CNN model\n",
    "        _training._train_model(__text_train_embedvec, __label_train_binary, \n",
    "                               __text_test_embedvec, __label_test_binary)\n",
    "        \n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mainscore\"></a>\n",
    "### MAIN: Score emotions of tweets and plot spider graphs.\n",
    "- Get production tweets using the [Database-Class](#database)\n",
    "- Score the tweets using the [Score-Class](#score)\n",
    "- Plot spider graphs showing the emotions using the [Plot-Class](#plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_and_plot():\n",
    "    # score emotions for tweets which are not yet scored\n",
    "    unscored_tweets = _db_client_production.get_unscored_tweets()\n",
    "    for tweet in unscored_tweets:\n",
    "        tweet['emotion'] = _score.score(tweet['full_text_processed'])\n",
    "    # persist the calculated emotions\n",
    "    _db_client_production.persist_tweets(unscored_tweets)\n",
    "    \n",
    "    # get emotions and its percentage respectively from database\n",
    "    percentage_emotions = _db_client_production.get_percentage_of_emotions()\n",
    "    # add current date\n",
    "    percentage_emotions.loc[len(percentage_emotions)] = [{'Datum': [datetime.today().\n",
    "                                                                    strftime('%b. \\'%y')]}]\n",
    "    \n",
    "    # plot\n",
    "    _plot = Plot(dataframe=percentage_emotions, graphtitle='AddTitle', scalegraph=40)\n",
    "    _plot.plot_graphs(percentage_emotions)\n",
    "    \n",
    "    \n",
    "score_and_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"database\"></a>\n",
    "### CLASS: Create the Database-API and manage the Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database:\n",
    "    def __init__(self,\n",
    "                 db_collection: str,\n",
    "                 db_name: str = \"textmining\",\n",
    "                 db_host='localhost',\n",
    "                 db_port: int = 27017\n",
    "                 ):\n",
    "        self._db_name = db_name\n",
    "        self._db_collection = db_collection\n",
    "        self._db_host = db_host\n",
    "        self._db_port = db_port\n",
    "        try:\n",
    "            # create database client for given parameters\n",
    "            self._client = self.__create_db_client()\n",
    "        except ConnectionFailure as e:\n",
    "            print('MongoDB error while establishing a connection : ', str(e))\n",
    "            raise\n",
    "\n",
    "    def __create_db_client(self):\n",
    "        try:\n",
    "            # create client connected to MongoDB\n",
    "            client = MongoClient(self._db_host, self._db_port)\n",
    "            # test if client is valid; else: exception is raised\n",
    "            client.server_info()\n",
    "            # return client for MongoDB database named 'db_name'\n",
    "            return client[self._db_name]\n",
    "        except ConnectionFailure:\n",
    "            raise\n",
    "\n",
    "    def persist_tweets(self,\n",
    "                       tweets: List[Dict,]\n",
    "                       ) -> None:\n",
    "        \"\"\"\n",
    "        Persists a list of dictionaries in the database where one dictionary represents one \n",
    "        Tweet in JSON format. If a specific user ID already has a persisted Tweet for a \n",
    "        specific week, the persisted Tweet will be overwritten. On the one hand this should \n",
    "        stem Twitter bots which do not give any valuable input for the system (i.e. bots\n",
    "        which hijack trending hashtags to post lots of advertising or the like). On the \n",
    "        other hand this should prevent an opinion bias since angry people tend to speak \n",
    "        out more frequently (i.e. one person = one opinion per week).\n",
    "        :param tweets: a list of dictionaries (represents Tweets in JSON format)\n",
    "        \"\"\"\n",
    "        # check if list of tweets is empty\n",
    "        if not tweets:\n",
    "            return\n",
    "\n",
    "        # Note: Unordered bulk write operations are batched and sent to the server in arbitrary \n",
    "        # order. Any errors (e.g. DuplicateKeyError) that occur are reported after ALL operations \n",
    "        # are attempted.\n",
    "        bulk = self._client[self._db_collection].initialize_unordered_bulk_op()\n",
    "\n",
    "        for tweet in tweets:\n",
    "            # Compare \"user id\" and \"created at\" from database with the Tweet to detect if the \n",
    "            # specific person already has stored a Tweet for the specific week. Update the stored \n",
    "            # Tweet with the new one if so. Note: Update if \"user id\" or \"created at\" is stored \n",
    "            # somewhere else.\n",
    "            bulk.find({\"$and\": [{\"user.id\": tweet[\"user\"][\"id\"]},\n",
    "                                {\"created_at_week_year\": tweet[\"created_at_week_year\"]}]}) \\\n",
    "                .upsert() \\\n",
    "                .update_one({\"$set\": tweet})\n",
    "        try:\n",
    "            # execute bulk operation\n",
    "            db_return = bulk.execute()\n",
    "\n",
    "            print('[', strftime(\"%d.%m.%Y %H:%M:%S\"), '] ', 'Database: ', 'Upserted(', \n",
    "                  db_return['nUpserted'], '), Modified(', db_return['nModified'],\n",
    "                  '), WriteErrors(', db_return['writeErrors'], ')', sep='')\n",
    "\n",
    "        except BulkWriteError as e:\n",
    "            print('MongoDB error while persisting Tweets : ', str(e.details))\n",
    "            raise\n",
    "\n",
    "    def get_unscored_tweets(self) -> Cursor:\n",
    "        # get cursor with all tweets from database where field 'emotion' does not exists yet\n",
    "        cursor = self._client[self._db_collection] \\\n",
    "            .find({'emotion': {\"$exists\": False}})\n",
    "        return cursor\n",
    "\n",
    "    def get_trainingdata(self, limit: int) -> pd.DataFrame:\n",
    "        # get data from database\n",
    "        cursor = self._client[self._db_collection] \\\n",
    "            .find({},\n",
    "                  {'full_text_processed': 1,\n",
    "                   'emotions': 1,\n",
    "                   \"_id\": 0}) \\\n",
    "            .limit(limit)\n",
    "\n",
    "        # load data into pandas DataFrame and expand the emotions into multiple rows:\n",
    "        # from:  | Lorem ipsum dolor  | [emoji_joy, emoji_love] |\n",
    "        # to:    | Lorem ipsum dolor  | emojy_joy  |\n",
    "        #        | Lorem ipsum dolor  | emojy_love |\n",
    "        df = pd.DataFrame(data=list(cursor))\n",
    "        df_expanded = pd.DataFrame([({'emotion': e, 'text': t.full_text_processed})\n",
    "                                    for t in df.itertuples()\n",
    "                                    for e in t.emotions])\n",
    "        # drop duplicate rows if neural network should consider only one label per text\n",
    "        # df_expanded.drop_duplicates(subset=['text'], inplace=True)\n",
    "        return df_expanded\n",
    "\n",
    "    def get_newest_tweet_id(self, tweet_id_column: str = 'id') -> Union[int, type(None)]:\n",
    "        \"\"\"\n",
    "        Returns the highest Tweet ID persisted in database. This ID represents the newest Tweet \n",
    "        processed so far.\n",
    "        :param tweet_id_column: the name of the database column containing the Tweet IDs\n",
    "        :returns: returns the highest Tweet ID persisted in database as Integer\n",
    "        :raises StopIteration: if no Tweet is yet persisted a StopIteration-Exception will be \n",
    "        raised\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Sort all entries for 'tweet_id' in descending order, limit the result to one, \n",
    "            # dereference the list of 'dict' with 'next()' to get the requested 'dict' and \n",
    "            # return only the value of 'tweet_id'.\n",
    "            tweet_id = self._client[self._db_collection].find() \\\n",
    "                .sort(tweet_id_column, DESCENDING) \\\n",
    "                .limit(1) \\\n",
    "                .next() \\\n",
    "                .get(tweet_id_column)\n",
    "            return tweet_id\n",
    "        except StopIteration:\n",
    "            # No tweet returned, i.e. database is empty.\n",
    "            return None\n",
    "\n",
    "    def get_percentage_of_emotions(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns a dataframe with emotions and its percentage respectively. For that it counts \n",
    "        all entries and each emotion separately in te database and calculates the percentage.\n",
    "        :return: dataframe with emotions and its percentage\n",
    "        \"\"\"\n",
    "        # create dataframe\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        try:\n",
    "            # get count of all entries where the field 'emotion' does exists\n",
    "            count_all = self._client[self._db_collection] \\\n",
    "                .find({'emotion': {\"$exists\": True}}).count()\n",
    "            \n",
    "            # query counts emotions and returns:\n",
    "            # { \"count\" : 370, \"emotion\" : \"joy\" }\n",
    "            # { \"count\" : 530, \"emotion\" : \"fear\" }\n",
    "            # etc.\n",
    "            for e in self._client[self._db_collection].aggregate(\n",
    "                        {\"$unwind\": \"$emotion\"},\n",
    "                        {\"$group\": {\"_id\": \"$emotion\", \"count\": {\"$sum\": 1}}},\n",
    "                        {\"$project\": {\"emotion\": \"$_id\", \"count\": 1}}\n",
    "                    ):\n",
    "                # calc the percentage of this emotion\n",
    "                percentage = (e['count']/count_all)*100\n",
    "                # write emotion with its percentage in the dataframe\n",
    "                df.loc[len(df)] = [e['emotion'], percentage]         \n",
    "        except StopIteration:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"twitter\"></a>\n",
    "### CLASS: Create the Twitter-API and to load Tweets based on a given search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Twitter:\n",
    "    def __init__(self,\n",
    "                 consumer_key: str = \"XXXX\",\n",
    "                 consumer_secret: str = \"XXXX\",\n",
    "                 tweets_per_query: int = 1000,\n",
    "                 ):\n",
    "        self._consumer_key = consumer_key\n",
    "        self._consumer_secret = consumer_secret\n",
    "        self._tweets_per_query = tweets_per_query\n",
    "\n",
    "        try:\n",
    "            # create Twitter API for given parameters\n",
    "            self._twitter_api = self.__create_twitter_api()\n",
    "        except TweepError as e:\n",
    "            print('Tweepy error while creating the API : ', str(e))\n",
    "            raise\n",
    "\n",
    "    def __create_twitter_api(self):\n",
    "        try:\n",
    "            # Create an AppAuthHandler instance and passing consumer token and secret.\n",
    "            # AppAuthHandler gives you higher limits than OAuthHandler.\n",
    "            auth = AppAuthHandler(self._consumer_key, self._consumer_secret)\n",
    "            # Create Tweepy API.\n",
    "            # 'wait_on_rate_limits' makes the Tweepy API call auto wait (sleep) when it hits \n",
    "            # the rate limit.\n",
    "            return API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True,\n",
    "                       retry_count=3, retry_delay=5, retry_errors={401, 404, 500, 503})\n",
    "        except TweepError:\n",
    "            raise\n",
    "\n",
    "    def load_tweets(self,\n",
    "                    search_query: str,\n",
    "                    since_id=None,\n",
    "                    max_id=None\n",
    "                    ) -> List[Dict,]:\n",
    "        \"\"\"\n",
    "        Loads matching Tweets for the given parameters by the given Twitter API.\n",
    "        :param search_query: the search query, consists of a search term and possible options\n",
    "                (e.g. \"#HelloWorld -filter:retweets\" which filters out all retweets)\n",
    "        :param since_id: the lowest tweet id considered, if 'none' go as far back as API allows\n",
    "        :param max_id: the highest tweet id considered, if 'none' start at most recent tweet\n",
    "        :return: a list of dictionaries where each dictionary represents one Tweet in JSON format\n",
    "        \"\"\"\n",
    "        tweets = []\n",
    "\n",
    "        try:\n",
    "            # Parameters for API.search cannot be provided directly into the method but has to be \n",
    "            # passed into the Cursor constructor method.\n",
    "            # Parameter: \"tweet_mode: 'extended'\" -> get full tweet text\n",
    "            for i, tweet in enumerate(Tweepy_Cursor(self._twitter_api.search, q=search_query,\n",
    "                                                    since_id=since_id, max_id=max_id,\n",
    "                                                    tweet_mode='extended')\n",
    "                                      .items(self._tweets_per_query)):\n",
    "                # Get Tweepy status object as json and append to list.\n",
    "                tweets.append(tweet._json)\n",
    "                stdout.write('\\r' + str(i + 1) + '    ')\n",
    "\n",
    "        except TweepError as e:\n",
    "            print('Tweepy error while getting Tweets : ', str(e))\n",
    "        except StopIteration:\n",
    "            raise\n",
    "\n",
    "        print('\\n', '[', strftime(\"%d.%m.%Y %H:%M:%S\"), '] ', 'Twitter: ', str(len(tweets)),\n",
    "              ' Tweets loaded', sep='')\n",
    "        return tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "### CLASS: Preprocess a given Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        # create emoji list once\n",
    "        self.__create_emoji_emotion_lists()\n",
    "\n",
    "    def preprocess_tweet(self, tweet: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Processes the given Tweet by:\n",
    "            - adding an item with 'created at' date formatted as 'week-year' \n",
    "              ('created_at_week_year')\n",
    "            - adding an item with summarized place information ('place_summarized')\n",
    "                - the x-y-coordinate\n",
    "                - the country\n",
    "                - the name and type of the exact location (e.g. Honolulu - City)\n",
    "            - adding an item with preprocessed text ('full_text_processed'):\n",
    "                - convert each character to lowercase\n",
    "                - remove tab, linefeed, carriage return\n",
    "                - remove the '#' from all hashtags\n",
    "                - remove all handles and URLs\n",
    "                - remove separating characters, e.g. this/that -> this that\n",
    "                - reduce repeating characters to max three characters, \n",
    "                  e.g. yaaaayyyyyy -> yaaayyy\n",
    "                - reduce repeating punctuation to one character, e.g. !!! -> !\n",
    "        :param tweet: a Tweet with related meta information given as json file (dictionary)\n",
    "        :return: the same Tweet with added items as json file (dictionary)\n",
    "        \"\"\"\n",
    "        tweet = self.__format_date_to_week_year(tweet)\n",
    "        tweet = self.__summarize_place_information(tweet)\n",
    "        tweet = self.__process_text(tweet)\n",
    "        return tweet\n",
    "\n",
    "    def preprocess_tweet_for_training(self, tweet: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Processes the given Tweet if it should be used as training data:\n",
    "            - execute the function 'preprocess_tweet(tweet: Dict)'\n",
    "            - assign the emojis in the tweet text to one of six emotions: anger, fear, sadness,\n",
    "              joy, love, surprise. The found emotions will be stored in the new item 'emotions' \n",
    "              and all emojis will be removed from text.\n",
    "            - if the Tweet text contains too less words (<7) the whole Tweet is set to 'None'.\n",
    "            - if the Tweet does not contain any emotions (i.e. no emoji can be assigned to an \n",
    "              emotion) the whole Tweet is set to 'None'.\n",
    "        :param tweet: a Tweet with related meta information given as json file (dictionary)\n",
    "        :return: the same Tweet with added items as json file (dictionary) or None\n",
    "        \"\"\"\n",
    "        tweet = self.preprocess_tweet(tweet)\n",
    "        tweet = self.__process_text_for_training(tweet)\n",
    "        tweet = self.__remove_unusable_tweet_for_training(tweet)\n",
    "        return tweet\n",
    "\n",
    "    @staticmethod\n",
    "    def __format_date_to_week_year(tweet: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Takes the 'created_at' date from the Tweet, formats it to 'week-year' and adds it\n",
    "        as a new item 'created_at_week_year' to the Tweet.\n",
    "        \"\"\"\n",
    "        # convert to datetime object, i.e. Fri Dec 29 18:16:29 +0000 2017\n",
    "        datetime_object = datetime.strptime(tweet['created_at'], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "        tweet['created_at_week_year'] = datetime_object.strftime(\"%V-%Y\")\n",
    "        return tweet\n",
    "\n",
    "    @staticmethod\n",
    "    def __summarize_place_information(tweet: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Constructs a dictionary containing summarized place information:\n",
    "            - the x-y-coordinate\n",
    "            - the country\n",
    "            - the name and type of the exact location (e.g. Honolulu - City)\n",
    "        and adds it as a new item 'place_summarized' to the Tweet.\n",
    "        \"\"\"\n",
    "        place = tweet['place']\n",
    "        if place:\n",
    "            # averaging (list of lists of) coordinates -> 4 coordinates gets averaged \n",
    "            # to 1 coordinate\n",
    "            avg_coordinates = [float(sum(col)) / len(col) for col in \n",
    "                               zip(*place['bounding_box']['coordinates'][0])]\n",
    "            x_coordinates = avg_coordinates[0]\n",
    "            y_coordinates = avg_coordinates[1]\n",
    "\n",
    "            tweet['place_summarized'] = {'country': place['country'], 'name': place['name'],\n",
    "                                         'place_type': place['place_type'],\n",
    "                                         'coordinates': [x_coordinates, y_coordinates]}\n",
    "        return tweet\n",
    "\n",
    "    @staticmethod\n",
    "    def __process_text(tweet: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Preprocesses the Tweet text:\n",
    "            - convert each character to lowercase\n",
    "            - remove tab, linefeed, carriage return\n",
    "            - remove the '#' from all hashtags\n",
    "            - remove all handles and URLs\n",
    "            - remove separating characters, e.g. this/that -> this that\n",
    "            - reduce repeating characters to max three characters, e.g. yaaaayyyyyy -> yaaayyy\n",
    "            - reduce repeating whitespaces to one whitespace\n",
    "            - reduce repeating punctuation to one character, e.g. !!! -> !\n",
    "        and adds it as new item 'full_text_processed' to the Tweet.\n",
    "        \"\"\"\n",
    "        # differentiate between retweeted and original Tweet;\n",
    "        # according to which is true the text has to be accessed from different items in \n",
    "        # the dictionary\n",
    "        if 'retweeted_status' in dir(tweet):\n",
    "            text = tweet['retweeted_status']['full_text']\n",
    "        else:\n",
    "            text = tweet['full_text']\n",
    "\n",
    "        # convert each character to lowercase\n",
    "        text = text.lower()\n",
    "        # remove tab, linefeed, carriage return\n",
    "        text = sub(r\"[\\t\\n\\r]\", r\" \", text)\n",
    "        # remove the '#' from all hashtags longer than three characters\n",
    "        text = sub(r\"\\#(\\w{4,})\", r\" \\1 \", text)\n",
    "        # remove all handles (i.e. any linkage to an username)\n",
    "        text = sub(r\"\\@(\\w+)\", \" \", text)\n",
    "        # remove all URLs\n",
    "        text = sub(r\"(http|https|ftp)://[a-zA-Z0-9\\\\./]+\", \" \", text)\n",
    "        # remove separating characters, e.g. this/that -> this that\n",
    "        text = sub(r\"[,\\\"\\:;\\(\\)\\[\\]\\{\\}&\\-_\\|/\\\\\\=~\\*]+\", \" \", text)\n",
    "        # reduce repeating characters to max three characters, e.g. yaaaayyyyyy -> yaaayyy\n",
    "        text = sub(r\"(.)\\1{3,}\", r\"\\1\\1\\1\", text)\n",
    "        # reduce repeating whitespaces to one whitespace\n",
    "        text = sub(r\"\\s{2,}\", \" \", text)\n",
    "        # reduce repeating punctuation which represents the end of a sentence to one character:\n",
    "        # '!!!' -> '!', '!' -> '!', '???' -> '?', '?' -> '?', '.' -> '.', etc\n",
    "        # but '...' -> '...' because '..' or '...' etc does not mark the end of a sentence\n",
    "        text = sub(r\"((?<!\\.)\\.(?!\\.))\\1|([\\!\\?])\\2+\", r\" \\1\\2 \", text)\n",
    "\n",
    "        tweet['full_text_processed'] = text\n",
    "        return tweet\n",
    "\n",
    "    def __process_text_for_training(self, tweet: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Processes the Tweet text so it can be used for training the neural network. For that \n",
    "        the emojis in the text will be assigned to one of six emotions: anger, fear, sadness, \n",
    "        joy, love, surprise. The found emotions will be stored in the new item 'emotions' \n",
    "        and all emojis will be removed from text.\n",
    "        \"\"\"\n",
    "        # make sure everything needed was created beforehand, i.e. correct order of function \n",
    "        # execution\n",
    "        assert 'full_text_processed' not in dir(tweet), \"Tweet does not have an item \" \\\n",
    "                                                        \"'full_text_processed' yet.\"\n",
    "        assert self._emoji_emotion_lists, \"Emoji list was not created yet.\"\n",
    "\n",
    "        # get the text\n",
    "        text = tweet['full_text_processed']\n",
    "        # initialise a set to store the emotions contained in the text (set ensures uniqueness)\n",
    "        emotions = set()\n",
    "\n",
    "        # iterate through the lists\n",
    "        for emotion, emoji_list in self._emoji_emotion_lists.items():\n",
    "            # for every emoji in the list ...\n",
    "            for emoji in emoji_list:\n",
    "                # ... check if its present in the text\n",
    "                if emoji in text:\n",
    "                    # remove emoji from text\n",
    "                    text = sub(emoji, \" \", text)\n",
    "                    # add this emotion\n",
    "                    emotions.add(emotion)\n",
    "\n",
    "        # remove skin tones which can be found in the text if an emoji with non standard skin \n",
    "        #  is removed\n",
    "        text = sub(r\"[üèªüèºüèΩüèæüèø]\", \" \", text)\n",
    "\n",
    "        tweet['full_text_processed'] = text\n",
    "        tweet['emotions'] = list(emotions)\n",
    "        return tweet\n",
    "\n",
    "    @staticmethod\n",
    "    def __remove_unusable_tweet_for_training(tweet: Dict) -> Union[Dict, type(None)]:\n",
    "        \"\"\"\n",
    "        Assigns Tweet to 'None' if the Tweet text\n",
    "        - contains too less words (<7); Tweets with too less words do not give any valuable \n",
    "          information for training\n",
    "        - does not contain any emotions (i.e. no emoji can be assigned to an emotion)\n",
    "        \"\"\"\n",
    "        # make sure everything needed was created beforehand, i.e. correct order of function \n",
    "        # execution\n",
    "        assert tweet['full_text_processed'], \"Tweet does not have an item \" \\\n",
    "                                             \"'full_text_processed' yet.\"\n",
    "\n",
    "        # check if text has less than 7 words\n",
    "        if len(tweet['full_text_processed'].split()) < 7:\n",
    "            return None\n",
    "\n",
    "        # check if text does not contain any emotions (i.e. list with emotions is empty)\n",
    "        if not tweet['emotions']:\n",
    "            return None\n",
    "\n",
    "        return tweet\n",
    "\n",
    "    def __create_emoji_emotion_lists(self):\n",
    "        # Basic Emotions by Parrott, W.: Emotions in Social Psychology. Psychology Press (2001)\n",
    "        # https://unicode.org/emoji/charts/full-emoji-list.html\n",
    "        emojis_anger = [\n",
    "            '>:/', '>:\\\\', '>:[', '>:(', ':@',\n",
    "            'üò§', 'üò°', 'üò†', 'üò£',\n",
    "            'ü§¢', 'ü§•', 'üí©', 'üëé', 'üôÖ', 'ü§Æ', 'ü§Æ',\n",
    "            'üôÑ'\n",
    "            # 'üò¨' = unfavorable situation but often used as grinning; better not use\n",
    "        ]\n",
    "        emojis_fear = [\n",
    "            'üòß', 'üò®', 'üò©', 'üò∞', 'üò±', 'üò≥', 'üòµ',\n",
    "            'üíÄ', '‚ò†', 'üë∫',\n",
    "        ]\n",
    "        emojis_sadness = [\n",
    "            # sad\n",
    "            ':-(', ')-:', ':(', '):', ':-[', ':[', ':-<', ':<', '=(', ':-/', ':/', '=/', ':L',\n",
    "            '=L', '=/',\n",
    "            ':S', ':\\\\', ':-c', ':c', ':{',\n",
    "            '‚òπ', 'üôÅ', 'üòñ', 'üòû', 'üòü', 'üò¶', 'üò•', 'üò´', 'üòí', 'üòì', 'üòï', 'üòê',\n",
    "            'üíî',\n",
    "            # crying\n",
    "            ':,(', \":'-(\", \":'(\", ':\"(', ':((',\n",
    "            'üò¢', 'üò≠',\n",
    "        ]\n",
    "        emojis_joy = [\n",
    "            # smiling\n",
    "            ':-)', ':-))', ':)', ':))', '(:', '(-:', '=)', '=]', ':o)', ':]', ':c)', ':>', '8)',\n",
    "            ':}', ':^)',\n",
    "            'üòä', 'üòé', '‚ò∫', 'üôÇ', 'ü§ó', 'ü§§', 'üòá',\n",
    "            # laughing/grinning\n",
    "            ':-D', ':D', 'D:', 'X-D', 'x-D', 'XD', 'xD', '=D', '8-D', '8D',\n",
    "            'üòÄ', 'üòÅ', 'üòÇ', 'ü§£', 'üòÉ', 'üòÑ', 'üòÖ', 'üòÜ', 'üòõ', 'üòú', 'üòù', 'üôÉ',\n",
    "            # winking\n",
    "            ';-D', ';D', ':-P', ':P', \":')\", ':-p', ':p', '=p', ':-b', ':b', 'üòã', 'üòè',\n",
    "            # ';-)', '(-;', ';)', '(;', 'üòâ', joy or sarcasm, better not use these\n",
    "            # other\n",
    "            'üëå', 'üëç', 'üéâ', 'üëè',\n",
    "        ]\n",
    "        emojis_love = [\n",
    "            # heart/kiss\n",
    "            '<3', ':3', ':*', ':^*',\n",
    "            '‚ù§', '‚ô•', 'üíï', 'üíñ', 'üíú', 'üíô', 'üíõ', 'üíö', 'üíó', 'üíò', 'üíû', 'üíã', 'üíì', 'üòç', \n",
    "            'üòò', 'üòó', 'üòô', 'üòö',\n",
    "            'üíè',\n",
    "        ]\n",
    "        emojis_surprise = [\n",
    "            'ü§î', 'ü§®', 'üòÆ', 'üòØ', 'üò≤'\n",
    "        ]\n",
    "\n",
    "        self._emoji_emotion_lists = {\"emojis_anger\": emojis_anger, \"emojis_fear\": emojis_fear,\n",
    "                                     \"emojis_sadness\": emojis_sadness, \"emojis_joy\": emojis_joy,\n",
    "                                     \"emojis_love\": emojis_love, \"emojis_surprise\": emojis_surprise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "### CLASS: Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self,\n",
    "                 n_gram: int = 2,\n",
    "                 vecsize: int = 300,  # word2vec has vector size of 300\n",
    "                 nb_filters: int = 1200,\n",
    "                 maxlen: int = 20\n",
    "                 ):\n",
    "        self.n_gram = n_gram\n",
    "        self.vecsize = vecsize\n",
    "        self.nb_filters = nb_filters\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_into_test_and_training(data) -> Tuple[np.ndarray, np.ndarray,\n",
    "                                                     np.ndarray, np.ndarray]:\n",
    "        return train_test_split(np.array(data.text), np.array(data.emotion),\n",
    "                                test_size=0.2)\n",
    "\n",
    "    def _texts_to_embedded_vec_matrices(self, text: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Converts the training texts to matrices of embedded vectors. Accordingly, each text\n",
    "        is a matrix of embedded vectors, whereby, each embedded vector represents one word \n",
    "        of the text. To convert a word to an embedded vector a pretrained word2vec model is \n",
    "        used. Therefore, each vector represents the position of the specific\n",
    "        word in the pretrained model (the used pretrained model consists of 300 dimensions).\n",
    "        :param text: the training texts as array\n",
    "        :return: the converted training data as matrix of matrices of embedded vectors\n",
    "        \"\"\"\n",
    "        text_embedvec = np.zeros(shape=(len(text), self.maxlen, self.vecsize))\n",
    "        for i in range(len(text)):\n",
    "            text[i] = word_tokenize(text[i].lower())  # tokenize text\n",
    "            for j in range(min(self.maxlen, len(text[i]))):  # maximal 'maxlen' words per text\n",
    "                text_embedvec[i, j] = self.__word_to_embedded_vec(text[i][j])\n",
    "        return text_embedvec\n",
    "\n",
    "    def __word_to_embedded_vec(self, word: str):\n",
    "        assert FAST_VERSION > -1, \"This will be slow otherwise\"\n",
    "        return _word2vec[word] if word in _word2vec else np.zeros(self.vecsize)\n",
    "\n",
    "    # convert labels to binary because nominal values cannot be used in training.\n",
    "    def _labels_to_binary(self, labels: np.ndarray) -> np.ndarray:\n",
    "        lb = preprocessing.LabelBinarizer()\n",
    "        lb.fit(labels)\n",
    "        # create lookup for used labels (prediction output has the same order as this list)\n",
    "        self.__classlabels = lb.classes_\n",
    "        return lb.transform(labels)\n",
    "\n",
    "    def _train_model(self, x_train, y_train, x_test, y_test):\n",
    "        # get number of unique labels (= number of different emotions)\n",
    "        nr_classes = len(y_train[0])\n",
    "        # build the deep neural network model\n",
    "        model = Sequential()\n",
    "        model.add(Convolution1D(nb_filter=self.nb_filters,\n",
    "                                filter_length=self.n_gram,\n",
    "                                border_mode='valid',\n",
    "                                activation='relu',\n",
    "                                input_shape=(self.maxlen, self.vecsize)))\n",
    "        model.add(MaxPooling1D(pool_length=self.maxlen - self.n_gram + 1))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(nr_classes, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['accuracy'])\n",
    "        # train and evaluate the model\n",
    "        model.fit(x_train, y_train, validation_data=(x_test, y_test))\n",
    "        # save the trained model on disc\n",
    "        model.save(_trained_model_savepath)\n",
    "        self.__model = model\n",
    "        # save class labels as json to assign labels back to the model if the model gets used.\n",
    "        with open(_classlabels_savepath, 'w') as fp:\n",
    "            json.dump(self.__classlabels, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"score\"></a>\n",
    "### CLASS: Score a given text based on the trained Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score:\n",
    "    def __init__(self,\n",
    "                 model: Sequential,  # trained Neural Network\n",
    "                 classlabels: dict,  # class labels to assign labels back to the model\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.classlabels = classlabels\n",
    "\n",
    "    def score(self, text: str):\n",
    "        # retrieve vector; note: 'ndimn=1' enforces an 1-D array because a 0-D array does \n",
    "        # not support len()-function\n",
    "        matrix = np.array([Training._texts_to_embedded_vec_matrices\n",
    "                           (np.array(text, ndmin=1, dtype=object))])\n",
    "        # remove extra dimension induced above\n",
    "        matrix_reduced = matrix[0, :, :, :]\n",
    "        # classification using the neural network\n",
    "        predictions = self.model.predict(matrix_reduced)\n",
    "        # assign predictions to their class labels\n",
    "        score = {}\n",
    "        for idx, classlabel in zip(range(len(self.classlabels)), self.classlabels):\n",
    "            score[classlabel] = predictions[0][idx]\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot\"></a>\n",
    "### CLASS: Plot spider graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plot:\n",
    "    def __init__(self,\n",
    "                 # dataframe contains the values to plot. each column gives one individual plot.\n",
    "                 # default value gives an example on how the dataframe has to look.\n",
    "                 dataframe: pd.DataFrame = pd.DataFrame({\n",
    "                                                'Datum': ['Nov. \\'17', 'Jan. \\'18'],\n",
    "                                                'Wut': [34, 28],\n",
    "                                                'Angst': [22, 16],\n",
    "                                                'Traurigkeit': [10, 14],\n",
    "                                                'Freude': [18, 26],\n",
    "                                                'Liebe': [10, 15],\n",
    "                                                '√úberraschung': [6, 7],\n",
    "                                            }),\n",
    "                 graphtitle: str = \"ExampleTitle\",  # title of graph\n",
    "                 scalegraph: int = 40,  # scale graph in %\n",
    "                 ):\n",
    "        self.df = dataframe\n",
    "        self.graphtitle = graphtitle\n",
    "        self.scalegraph = scalegraph\n",
    "     \n",
    "    @staticmethod\n",
    "    def plot_graphs(self):\n",
    "        # apply to all individuals. initialize the figure.\n",
    "        my_dpi = 96\n",
    "        plt.figure(figsize=(1000 / my_dpi, 1000 / my_dpi), dpi=my_dpi)\n",
    "        \n",
    "        # create a color palette. the different graphs will get created using a color of\n",
    "        # the specified color palette.\n",
    "        my_palette = plt.cm.get_cmap(\"Spectral\", len(self.df.index))\n",
    "        \n",
    "        # loop to plot\n",
    "        for row in range(0, len(self.df.index)):\n",
    "            self.make_one_graph(row=row, title=self.df['Datum'][row], color=my_palette(row))\n",
    "        \n",
    "        # show graph\n",
    "        plt.show(block=True) \n",
    "      \n",
    "    # function to plot one column of the dataset\n",
    "    def make_one_graph(self, row, title, color):\n",
    "        # number of variable\n",
    "        categories = list(self.df)\n",
    "        categories.remove('Datum')\n",
    "        N = len(categories)\n",
    "    \n",
    "        # what will be the angle of each axis in the plot? (divide the plot / number of variable)\n",
    "        angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "        angles += angles[:1]\n",
    "    \n",
    "        # initialise the spider plot\n",
    "        ax = plt.subplot(2, 2, row + 1, polar=True, )\n",
    "    \n",
    "        # first axis to be on top\n",
    "        ax.set_theta_offset(pi / 2)\n",
    "        ax.set_theta_direction(-1)\n",
    "    \n",
    "        # draw one axe per variable + add labels labels yet\n",
    "        plt.xticks(angles[:-1], categories, color='#4c4c4c', size=8, fontweight='bold')\n",
    "    \n",
    "        # draw ylabels\n",
    "        ax.set_rlabel_position(0)\n",
    "        plt.yticks([10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
    "                   [\"10%\", \"20%\", \"30%\", \"40%\", \"50%\", \"60%\", \"70%\", \"80%\", \"90%\"],\n",
    "                   color=\"#4c4c4c\", size=7)\n",
    "        plt.ylim(0, self.scalegraph)\n",
    "    \n",
    "        # ind1\n",
    "        values = self.df.loc[row].drop('Datum').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, color=color, linewidth=2, linestyle='solid')\n",
    "        ax.fill(angles, values, color=color, alpha=0.4)\n",
    "    \n",
    "        plt.suptitle(self.graphtitle, fontsize=14, color='#4c4c4c')\n",
    "    \n",
    "        # add a graph title\n",
    "        plt.title(title, size=11, color=color, y=1.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"helptweets\"></a>\n",
    "### HELPER-CLASS: Get informations about Tweets in database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB\n",
    "db_host = \"localhost\"\n",
    "db_port = 27017\n",
    "db_name = \"textmining\"\n",
    "db_collection = \"training_tweets\"\n",
    "\n",
    "# create client connected to MongoDB\n",
    "client = MongoClient(db_host, db_port)\n",
    "# return client for MongoDB collection named 'db_name'\n",
    "db = client[db_name]\n",
    "\n",
    "# count tweets in database\n",
    "pprint.pprint(\"Tweets in Database: \" + str(db[db_collection].count()))\n",
    "# print informations about tweets in database\n",
    "for tweet in db[db_collection].find():\n",
    "    pprint.pprint(\"Tweet-ID: \" + str(tweet[\"id\"]))\n",
    "    #pprint.pprint(\"Tweet-Text: \" + str(tweet[\"full_text\"]))\n",
    "    #pprint.pprint(\"Tweet-Text-Processed: \" + str(tweet[\"full_text_processed\"]))\n",
    "    pprint.pprint(\"Tweet-Emotions: \" + str(tweet[\"emotions\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"helpdelete\"></a>\n",
    "### HELPER-CLASS: Delete a given Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB\n",
    "db_host = \"localhost\"\n",
    "db_port = 27017\n",
    "db_name = \"textmining\"\n",
    "db_collection = \"training_tweets\"\n",
    "\n",
    "\n",
    "# create client connected to MongoDB\n",
    "client = MongoClient(db_host, db_port)\n",
    "# return client for MongoDB collection named 'db_name'\n",
    "db = client[db_name]\n",
    "\n",
    "print(db[db_collection].delete_many({}).deleted_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
